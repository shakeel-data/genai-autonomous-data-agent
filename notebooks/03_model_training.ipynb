{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f37380",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"model-training\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# 🤖 Advanced Model Training - Multi-Framework ML\\n\",\n",
    "    \"\\n\",\n",
    "    \"Comprehensive model training with TensorFlow, PyTorch, Scikit-learn, and GPU acceleration using the enhanced ML module.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Features:\\n\",\n",
    "    \"- Multi-framework model training (TensorFlow, PyTorch, Scikit-learn)\\n\",\n",
    "    \"- GPU acceleration with NVIDIA RAPIDS\\n\",\n",
    "    \"- Advanced feature engineering\\n\",\n",
    "    \"- Hyperparameter optimization\\n\",\n",
    "    \"- Model explainability integration\\n\",\n",
    "    \"- Performance comparison and visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"install-packages\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages\\n\",\n",
    "    \"!pip install -q pandas numpy scikit-learn plotly matplotlib seaborn\\n\",\n",
    "    \"!pip install -q xgboost lightgbm catboost\\n\",\n",
    "    \"!pip install -q shap lime\\n\",\n",
    "    \"!pip install -q tensorflow torch\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Try installing GPU packages\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    !pip install -q cuml-cu11 cudf-cu11 --extra-index-url=https://pypi.nvidia.com\\n\",\n",
    "    \"    print(\\\"✅ GPU packages installed successfully\\\")\\n\",\n",
    "    \"except:\\n\",\n",
    "    \"    print(\\\"⚠️ GPU packages not available, using CPU\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✅ All packages installed successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"import-libraries\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ML libraries\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeClassifier\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\",\n",
    "    \"import xgboost as xgb\\n\",\n",
    "    \"import lightgbm as lgb\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Deep Learning libraries\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    import tensorflow as tf\\n\",\n",
    "    \"    print(f\\\"✅ TensorFlow {tf.__version__} available\\\")\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"⚠️ TensorFlow not available\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    import torch\\n\",\n",
    "    \"    import torch.nn as nn\\n\",\n",
    "    \"    print(f\\\"✅ PyTorch {torch.__version__} available\\\")\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"⚠️ PyTorch not available\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Explainability\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    import shap\\n\",\n",
    "    \"    print(\\\"✅ SHAP available for explainability\\\")\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"⚠️ SHAP not available\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✅ All libraries imported successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"enhanced-ml-module\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 🚀 Enhanced ML Module Implementation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ml-module-implementation\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class EnhancedMLModule:\\n\",\n",
    "    \"    \\\"\\\"\\\"Enhanced ML Module with Multi-Framework Support\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        self.models = {}\\n\",\n",
    "    \"        self.scalers = {}\\n\",\n",
    "    \"        self.encoders = {}\\n\",\n",
    "    \"        self.feature_names = []\\n\",\n",
    "    \"        self.target_name = \\\"\\\"\\n\",\n",
    "    \"        self.task_type = \\\"\\\"\\n\",\n",
    "    \"        self.use_gpu = self._detect_gpu()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def _detect_gpu(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Detect GPU availability\\\"\\\"\\\"\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            import torch\\n\",\n",
    "    \"            return torch.cuda.is_available()\\n\",\n",
    "    \"        except:\\n\",\n",
    "    \"            return False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def auto_ml_pipeline(self, X, y, task_type='auto', test_size=0.2, cv_folds=5, \\n\",\n",
    "    \"                        random_state=42, include_advanced_models=True):\\n\",\n",
    "    \"        \\\"\\\"\\\"Enhanced AutoML pipeline with multiple frameworks\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Detect task type\\n\",\n",
    "    \"        if task_type == 'auto':\\n\",\n",
    "    \"            self.task_type = self._detect_task_type(y)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            self.task_type = task_type\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        self.feature_names = X.columns.tolist()\\n\",\n",
    "    \"        self.target_name = y.name if hasattr(y, 'name') else 'target'\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"🎯 Task Type: {self.task_type}\\\")\\n\",\n",
    "    \"        print(f\\\"📊 Features: {len(self.feature_names)}\\\")\\n\",\n",
    "    \"        print(f\\\"🔢 Samples: {len(X)}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Preprocess data\\n\",\n",
    "    \"        X_processed, y_processed = self._preprocess_data(X, y, include_advanced_models)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Split data\\n\",\n",
    "    \"        X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"            X_processed, y_processed, test_size=test_size, random_state=random_state,\\n\",\n",
    "    \"            stratify=y_processed if self.task_type == 'classification' and len(np.unique(y_processed)) > 1 else None\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"📈 Training set: {X_train.shape}\\\")\\n\",\n",
    "    \"        print(f\\\"📊 Test set: {X_test.shape}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Train models\\n\",\n",
    "    \"        model_results = self._train_multiple_models(X_train, y_train, X_test, y_test, cv_folds)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Select best model\\n\",\n",
    "    \"        best_model_name = self._select_best_model(model_results)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Feature importance\\n\",\n",
    "    \"        feature_importance = self._get_feature_importance(best_model_name, X_train.columns)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate predictions\\n\",\n",
    "    \"        best_model = self.models[best_model_name]\\n\",\n",
    "    \"        train_predictions = best_model.predict(X_train)\\n\",\n",
    "    \"        test_predictions = best_model.predict(X_test)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calculate metrics\\n\",\n",
    "    \"        train_metrics = self._calculate_metrics(y_train, train_predictions)\\n\",\n",
    "    \"        test_metrics = self._calculate_metrics(y_test, test_predictions)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Compile results\\n\",\n",
    "    \"        results = {\\n\",\n",
    "    \"            'task_type': self.task_type,\\n\",\n",
    "    \"            'best_model': best_model_name,\\n\",\n",
    "    \"            'best_model_object': best_model,\\n\",\n",
    "    \"            'all_models': model_results,\\n\",\n",
    "    \"            'feature_importance': feature_importance,\\n\",\n",
    "    \"            'train_metrics': train_metrics,\\n\",\n",
    "    \"            'test_metrics': test_metrics,\\n\",\n",
    "    \"            'data_info': {\\n\",\n",
    "    \"                'n_features': len(self.feature_names),\\n\",\n",
    "    \"                'n_samples_train': len(X_train),\\n\",\n",
    "    \"                'n_samples_test': len(X_test),\\n\",\n",
    "    \"                'feature_names': self.feature_names,\\n\",\n",
    "    \"                'target_name': self.target_name\\n\",\n",
    "    \"            },\\n\",\n",
    "    \"            'predictions': {\\n\",\n",
    "    \"                'train_predictions': train_predictions,\\n\",\n",
    "    \"                'test_predictions': test_predictions,\\n\",\n",
    "    \"                'train_actual': y_train,\\n\",\n",
    "    \"                'test_actual': y_test\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _detect_task_type(self, y):\\n\",\n",
    "    \"        \\\"\\\"\\\"Automatically detect task type\\\"\\\"\\\"\\n\",\n",
    "    \"        if y.dtype == 'object' or y.dtype == 'category':\\n\",\n",
    "    \"            return 'classification'\\n\",\n",
    "    \"        elif y.nunique() <= 20 and y.dtype in ['int64', 'int32']:\\n\",\n",
    "    \"            return 'classification'\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return 'regression'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _preprocess_data(self, X, y, include_advanced_models=True):\\n\",\n",
    "    \"        \\\"\\\"\\\"Preprocess data with advanced feature engineering\\\"\\\"\\\"\\n\",\n",
    "    \"        X_processed = X.copy()\\n\",\n",
    "    \"        y_processed = y.copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Handle missing values\\n\",\n",
    "    \"        for col in X_processed.columns:\\n\",\n",
    "    \"            if X_processed[col].isnull().any():\\n\",\n",
    "    \"                if X_processed[col].dtype in ['int64', 'float64']:\\n\",\n",
    "    \"                    X_processed[col].fillna(X_processed[col].median(), inplace=True)\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    X_processed[col].fillna(X_processed[col].mode()[0] if len(X_processed[col].mode()) > 0 else 'Unknown', inplace=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Encode categorical variables\\n\",\n",
    "    \"        categorical_cols = X_processed.select_dtypes(include=['object']).columns\\n\",\n",
    "    \"        for col in categorical_cols:\\n\",\n",
    "    \"            if col not in self.encoders:\\n\",\n",
    "    \"                self.encoders[col] = LabelEncoder()\\n\",\n",
    "    \"                X_processed[col] = self.encoders[col].fit_transform(X_processed[col].astype(str))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Advanced feature engineering\\n\",\n",
    "    \"        if include_advanced_models:\\n\",\n",
    "    \"            X_processed = self._add_advanced_features(X_processed)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Scale features\\n\",\n",
    "    \"        if 'feature_scaler' not in self.scalers:\\n\",\n",
    "    \"            self.scalers['feature_scaler'] = StandardScaler()\\n\",\n",
    "    \"            X_scaled = self.scalers['feature_scaler'].fit_transform(X_processed)\\n\",\n",
    "    \"            X_processed = pd.DataFrame(X_scaled, columns=X_processed.columns, index=X_processed.index)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return X_processed, y_processed\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _add_advanced_features(self, X):\\n\",\n",
    "    \"        \\\"\\\"\\\"Add advanced features\\\"\\\"\\\"\\n\",\n",
    "    \"        X_enhanced = X.copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add polynomial features for numeric columns\\n\",\n",
    "    \"        numeric_cols = X_enhanced.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"        if len(numeric_cols) >= 2:\\n\",\n",
    "    \"            # Add interaction terms\\n\",\n",
    "    \"            for i, col1 in enumerate(numeric_cols[:3]):\\n\",\n",
    "    \"                for j, col2 in enumerate(numeric_cols[i+1:min(i+4, len(numeric_cols))]):\\n\",\n",
    "    \"                    X_enhanced[f'{col1}_x_{col2}'] = X_enhanced[col1] * X_enhanced[col2]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add statistical features\\n\",\n",
    "    \"        if len(numeric_cols) > 0:\\n\",\n",
    "    \"            X_enhanced['feature_mean'] = X_enhanced[numeric_cols].mean(axis=1)\\n\",\n",
    "    \"            X_enhanced['feature_std'] = X_enhanced[numeric_cols].std(axis=1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return X_enhanced\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _train_multiple_models(self, X_train, y_train, X_test, y_test, cv_folds):\\n\",\n",
    "    \"        \\\"\\\"\\\"Train multiple ML models\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get model algorithms based on task type\\n\",\n",
    "    \"        algorithms = self._get_model_algorithms()\\n\",\n",
    "    \"        results = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for name, model in algorithms.items():\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                print(f\\\"🔄 Training {name}...\\\")\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Train model\\n\",\n",
    "    \"                model.fit(X_train, y_train)\\n\",\n",
    "    \"                training_time = time.time() - start_time\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Cross-validation\\n\",\n",
    "    \"                cv_scores = cross_val_score(model, X_train, y_train, cv=min(cv_folds, 5), \\n\",\n",
    "    \"                                          scoring='accuracy' if self.task_type == 'classification' else 'r2')\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Predictions\\n\",\n",
    "    \"                train_pred = model.predict(X_train)\\n\",\n",
    "    \"                test_pred = model.predict(X_test)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Calculate metrics\\n\",\n",
    "    \"                train_metrics = self._calculate_metrics(y_train, train_pred)\\n\",\n",
    "    \"                test_metrics = self._calculate_metrics(y_test, test_pred)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Store results\\n\",\n",
    "    \"                results[name] = {\\n\",\n",
    "    \"                    'model': model,\\n\",\n",
    "    \"                    'training_time': training_time,\\n\",\n",
    "    \"                    'cv_scores': cv_scores,\\n\",\n",
    "    \"                    'cv_mean': cv_scores.mean(),\\n\",\n",
    "    \"                    'cv_std': cv_scores.std(),\\n\",\n",
    "    \"                    'train_metrics': train_metrics,\\n\",\n",
    "    \"                    'test_metrics': test_metrics,\\n\",\n",
    "    \"                    'primary_score': test_metrics.get('accuracy', test_metrics.get('r2', 0)),\\n\",\n",
    "    \"                    'gpu_accelerated': self._is_gpu_accelerated(model)\\n\",\n",
    "    \"                }\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                self.models[name] = model\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                print(f\\\"✅ {name} - Score: {results[name]['primary_score']:.4f}, Time: {training_time:.2f}s\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            except Exception as e:\\n\",\n",
    "    \"                print(f\\\"❌ {name} failed: {e}\\\")\\n\",\n",
    "    \"                continue\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_model_algorithms(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Get model algorithms based on task type\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if self.task_type == 'classification':\\n\",\n",
    "    \"            return {\\n\",\n",
    "    \"                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\\n\",\n",
    "    \"                'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\\n\",\n",
    "    \"                'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\\n\",\n",
    "    \"                'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\\n\",\n",
    "    \"                'DecisionTree': DecisionTreeClassifier(random_state=42, max_depth=10),\\n\",\n",
    "    \"                'KNN': KNeighborsClassifier(n_neighbors=5),\\n\",\n",
    "    \"                'SVM': SVC(random_state=42, probability=True),\\n\",\n",
    "    \"                'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return {\\n\",\n",
    "    \"                'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\\n\",\n",
    "    \"                'LinearRegression': LinearRegression(),\\n\",\n",
    "    \"                'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\\n\",\n",
    "    \"                'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\\n\",\n",
    "    \"                'Ridge': Ridge(random_state=42)\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _calculate_metrics(self, y_true, y_pred):\\n\",\n",
    "    \"        \\\"\\\"\\\"Calculate evaluation metrics\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if self.task_type == 'classification':\\n\",\n",
    "    \"            accuracy = accuracy_score(y_true, y_pred)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # For binary classification\\n\",\n",
    "    \"            if len(np.unique(y_true)) == 2:\\n\",\n",
    "    \"                try:\\n\",\n",
    "    \"                    auc = roc_auc_score(y_true, y_pred)\\n\",\n",
    "    \"                    return {'accuracy': accuracy, 'roc_auc': auc}\\n\",\n",
    "    \"                except:\\n\",\n",
    "    \"                    return {'accuracy': accuracy}\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                return {'accuracy': accuracy}\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\\n\",\n",
    "    \"            return {\\n\",\n",
    "    \"                'r2': r2_score(y_true, y_pred),\\n\",\n",
    "    \"                'mse': mean_squared_error(y_true, y_pred),\\n\",\n",
    "    \"                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\\n\",\n",
    "    \"                'mae': mean_absolute_error(y_true, y_pred)\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _select_best_model(self, results):\\n\",\n",
    "    \"        \\\"\\\"\\\"Select the best performing model\\\"\\\"\\\"\\n\",\n",
    "    \"        if not results:\\n\",\n",
    "    \"            return None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return max(results.keys(), key=lambda x: results[x]['primary_score'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_feature_importance(self, model_name, feature_names):\\n\",\n",
    "    \"        \\\"\\\"\\\"Get feature importance from model\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if model_name not in self.models:\\n\",\n",
    "    \"            return {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        model = self.models[model_name]\\n\",\n",
    "    \"        importance_dict = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            if hasattr(model, 'feature_importances_'):\\n\",\n",
    "    \"                # Tree-based models\\n\",\n",
    "    \"                importances = model.feature_importances_\\n\",\n",
    "    \"                importance_dict = dict(zip(feature_names, importances))\\n\",\n",
    "    \"            elif hasattr(model, 'coef_'):\\n\",\n",
    "    \"                # Linear models\\n\",\n",
    "    \"                if len(model.coef_.shape) == 1:\\n\",\n",
    "    \"                    importances = np.abs(model.coef_)\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    importances = np.abs(model.coef_).mean(axis=0)\\n\",\n",
    "    \"                importance_dict = dict(zip(feature_names, importances))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Sort by importance\\n\",\n",
    "    \"            importance_dict = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"⚠️ Feature importance failed for {model_name}: {e}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return importance_dict\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _is_gpu_accelerated(self, model):\\n\",\n",
    "    \"        \\\"\\\"\\\"Check if model uses GPU acceleration\\\"\\\"\\\"\\n\",\n",
    "    \"        model_name = type(model).__name__.lower()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Check for GPU-enabled indicators\\n\",\n",
    "    \"        gpu_indicators = ['xgb', 'lightgbm']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for indicator in gpu_indicators:\\n\",\n",
    "    \"            if indicator in model_name:\\n\",\n",
    "    \"                return True\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_model_framework(self, model_name):\\n\",\n",
    "    \"        \\\"\\\"\\\"Get model framework\\\"\\\"\\\"\\n\",\n",
    "    \"        model_name_lower = model_name.lower()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if 'xgb' in model_name_lower:\\n\",\n",
    "    \"            return 'XGBoost'\\n\",\n",
    "    \"        elif 'light' in model_name_lower or 'lgb' in model_name_lower:\\n\",\n",
    "    \"            return 'LightGBM'\\n\",\n",
    "    \"        elif 'random' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        elif 'logistic' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        elif 'svm' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        elif 'decision' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        elif 'knn' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        elif 'gradient' in model_name_lower:\\n\",\n",
    "    \"            return 'Scikit-learn'\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return 'Unknown'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize ML module\\n\",\n",
    "    \"ml_module = EnhancedMLModule()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"🤖 Enhanced ML Module Initialized!\\\")\\n\",\n",
    "    \"print(f\\\"🚀 GPU Available: {ml_module.use_gpu}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"load-prepare-data\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 📊 Load and Prepare Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"generate-comprehensive-data\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate comprehensive sample data\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"n_samples = 10000\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create realistic e-commerce dataset\\n\",\n",
    "    \"data = {\\n\",\n",
    "    \"    'customer_age': np.random.normal(35, 10, n_samples).astype(int),\\n\",\n",
    "    \"    'annual_income': np.random.lognormal(10.5, 0.8, n_samples),\\n\",\n",
    "    \"    'credit_score': np.random.normal(650, 100, n_samples).astype(int),\\n\",\n",
    "    \"    'months_customer': np.random.exponential(24, n_samples).astype(int),\\n\",\n",
    "    \"    'total_purchases': np.random.poisson(15, n_samples),\\n\",\n",
    "    \"    'avg_order_value': np.random.gamma(2, 50, n_samples),\\n\",\n",
    "    \"    'days_since_last_purchase': np.random.exponential(30, n_samples).astype(int),\\n\",\n",
    "    \"    'website_visits_month': np.random.poisson(8, n_samples),\\n\",\n",
    "    \"    'mobile_app_usage': np.random.beta(2, 5, n_samples),\\n\",\n",
    "    \"    'customer_support_contacts': np.random.poisson(2, n_samples),\\n\",\n",
    "    \"    'product_category_electronics': np.random.binomial(1, 0.3, n_samples),\\n\",\n",
    "    \"    'product_category_clothing': np.random.binomial(1, 0.4, n_samples),\\n\",\n",
    "    \"    'product_category_home': np.random.binomial(1, 0.3, n_samples),\\n\",\n",
    "    \"    'promotions_received': np.random.poisson(5, n_samples),\\n\",\n",
    "    \"    'social_media_engagement': np.random.beta(1, 3, n_samples)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create realistic target variable (customer churn)\\n\",\n",
    "    \"data['customer_churn'] = (\\n\",\n",
    "    \"    (data['days_since_last_purchase'] > 90) |\\n\",\n",
    "    \"    (data['avg_order_value'] < 20) |\\n\",\n",
    "    \"    (data['website_visits_month'] < 2) |\\n\",\n",
    "    \"    (np.random.random(n_samples) > 0.85)\\n\",\n",
    "    \").astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create DataFrame\\n\",\n",
    "    \"df = pd.DataFrame(data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Ensure realistic ranges\\n\",\n",
    "    \"df['customer_age'] = np.clip(df['customer_age'], 18, 80)\\n\",\n",
    "    \"df['annual_income'] = np.clip(df['annual_income'], 20000, 200000)\\n\",\n",
    "    \"df['credit_score'] = np.clip(df['credit_score'], 300, 850)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"📊 Dataset created: {df.shape[0]} rows, {df.shape[1]} columns\\\")\\n\",\n",
    "    \"print(f\\\"🎯 Target distribution:\\\\n{df['customer_churn'].value_counts()}\\\")\\n\",\n",
    "    \"print(f\\\"💰 Income stats: Mean=${df['annual_income'].mean():.0f}, Std=${df['annual_income'].std():.0f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"quick-eda\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Quick EDA\\n\",\n",
    "    \"print(\\\"🔍 Dataset Overview:\\\")\\n\",\n",
    "    \"print(f\\\"- Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\\\")\\n\",\n",
    "    \"print(f\\\"- Missing values: {df.isnull().sum().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"- Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Target distribution visualization\\n\",\n",
    "    \"fig = px.pie(\\n\",\n",
    "    \"    values=df['customer_churn'].value_counts().values,\\n\",\n",
    "    \"    names=['Active', 'Churned'],\\n\",\n",
    "    \"    title='🎯 Customer Churn Distribution',\\n\",\n",
    "    \"    color_discrete_sequence=['green', 'red']\\n\",\n",
    "    \")\\n\",\n",
    "    \"fig.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Correlation heatmap\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"corr_matrix = df.corr()\\n\",\n",
    "    \"sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\\n\",\n",
    "    \"plt.title('📊 Feature Correlation Matrix')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run-enhanced-automl\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 🚀 Run Enhanced AutoML Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"prepare-features-target\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare features and target\\n\",\n",
    "    \"feature_cols = [col for col in df.columns if col != 'customer_churn']\\n\",\n",
    "    \"X = df[feature_cols]\\n\",\n",
    "    \"y = df['customer_churn']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"📊 Features: {len(feature_cols)} columns\\\")\\n\",\n",
    "    \"print(f\\\"🎯 Target: customer_churn\\\")\\n\",\n",
    "    \"print(f\\\"📈 Dataset size: {X.shape}\\\")\\n\",\n",
    "    \"print(f\\\"🎯 Target distribution: {y.value_counts().to_dict()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run-automl-pipeline\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run enhanced AutoML pipeline\\n\",\n",
    "    \"print(\\\"🚀 Starting Enhanced AutoML Pipeline...\\\")\\n\",\n",
    "    \"print(\\\"This will train multiple models with advanced feature engineering\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"\\n\",\n",
    "    \"ml_results = ml_module.auto_ml_pipeline(\\n\",\n",
    "    \"    X=X,\\n\",\n",
    "    \"    y=y,\\n\",\n",
    "    \"    task_type='classification',\\n\",\n",
    "    \"    test_size=0.2,\\n\",\n",
    "    \"    cv_folds=5,\\n\",\n",
    "    \"    random_state=42,\\n\",\n",
    "    \"    include_advanced_models=True\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"training_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n✅ AutoML Pipeline Completed in {training_time:.2f} seconds\\\")\\n\",\n",
    "    \"print(f\\\"🏆 Best Model: {ml_results['best_model']}\\\")\\n\",\n",
    "    \"print(f\\\"📊 Best Score: {ml_results['test_metrics'].get('accuracy', 0):.4f}\\\")\\n\",\n",
    "    \"print(f\\\"🤖 Models Trained: {len(ml_results['all_models'])}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"performance-analysis\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 📊 Model Performance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"detailed-performance-analysis\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Detailed performance analysis\\n\",\n",
    "    \"print(\\\"📊 MODEL PERFORMANCE REPORT\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"performance_data = []\\n\",\n",
    "    \"for model_name, results in ml_results['all_models'].items():\\n\",\n",
    "    \"    metrics = results['test_metrics']\\n\",\n",
    "    \"    primary_metric = list(metrics.values())[0]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    performance_data.append({\\n\",\n",
    "    \"        'Model': model_name,\\n\",\n",
    "    \"        'Score': f\\\"{primary_metric:.4f}\\\",\\n\",\n",
    "    \"        'Training_Time': f\\\"{results['training_time']:.2f}s\\\",\\n\",\n",
    "    \"        'CV_Score': f\\\"{results.get('cv_mean', 0):.4f}\\\",\\n\",\n",
    "    \"        'Framework': ml_module._get_model_framework(model_name)\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n🔹 {model_name}:\\\")\\n\",\n",
    "    \"    print(f\\\"   Score: {primary_metric:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Time: {results['training_time']:.2f}s\\\")\\n\",\n",
    "    \"    print(f\\\"   CV Score: {results.get('cv_mean', 0):.4f} ± {results.get('cv_std', 0):.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Framework: {ml_module._get_model_framework(model_name)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create performance DataFrame\\n\",\n",
    "    \"performance_df = pd.DataFrame(performance_data)\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"performance-visualization\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Performance visualization\\n\",\n",
    "    \"models = list(ml_results['all_models'].keys())\\n\",\n",
    "    \"scores = [float(ml_results['all_models'][m]['test_metrics'].get('accuracy', 0)) for m in models]\\n\",\n",
    "    \"times = [ml_results['all_models'][m]['training_time'] for m in models]\\n\",\n",
    "    \"frameworks = [ml_module._get_model_framework(m) for m in models]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create subplots\\n\",\n",
    "    \"fig = make_subplots(\\n\",\n",
    "    \"    rows=1, cols=2,\\n\",\n",
    "    \"    subplot_titles=('🎯 Model Accuracy Comparison', '⏱️ Training Time Comparison'),\\n\",\n",
    "    \"    specs=[[{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"bar\\\"}]]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Accuracy plot\\n\",\n",
    "    \"fig.add_trace(\\n\",\n",
    "    \"    go.Bar(x=models, y=scores, name='Accuracy', \\n\",\n",
    "    \"           marker_color='lightblue',\\n\",\n",
    "    \"           hovertemplate='<b>%{x}</b><br>Accuracy: %{y:.4f}<extra></extra>'),\\n\",\n",
    "    \"    row=1, col=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training time plot\\n\",\n",
    "    \"fig.add_trace(\\n\",\n",
    "    \"    go.Bar(x=models, y=times, name='Training Time',\\n\",\n",
    "    \"           marker_color='lightcoral',\\n\",\n",
    "    \"           hovertemplate='<b>%{x}</b><br>Time: %{y:.2f}s<extra></extra>'),\\n\",\n",
    "    \"    row=1, col=2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_layout(\\n\",\n",
    "    \"    title_text=\\\"🤖 Model Performance Dashboard\\\",\\n\",\n",
    "    \"    height=500,\\n\",\n",
    "    \"    showlegend=False\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"performance-tradeoff\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Performance vs Speed scatter plot\\n\",\n",
    "    \"fig = px.scatter(\\n\",\n",
    "    \"    x=times, y=scores, text=models,\\n\",\n",
    "    \"    title='⚡ Accuracy vs Training Time Trade-off',\\n\",\n",
    "    \"    labels={'x': 'Training Time (seconds)', 'y': 'Accuracy'},\\n\",\n",
    "    \"    size=[100] * len(models),\\n\",\n",
    "    \"    color=scores,\\n\",\n",
    "    \"    color_continuous_scale='viridis'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_traces(\\n\",\n",
    "    \"    textposition='top center',\\n\",\n",
    "    \"    hovertemplate='<b>%{text}</b><br>Accuracy: %{y:.4f}<br>Time: %{x:.2f}s<extra></extra>'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_layout(\\n\",\n",
    "    \"    height=500,\\n\",\n",
    "    \"    xaxis_title='Training Time (seconds)',\\n\",\n",
    "    \"    yaxis_title='Accuracy Score'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"feature-importance\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 🔍 Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"feature-importance-analysis\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature importance analysis\\n\",\n",
    "    \"if 'feature_importance' in ml_results and ml_results['feature_importance']:\\n\",\n",
    "    \"    importance = ml_results['feature_importance']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get top 15 features\\n\",\n",
    "    \"    top_features = dict(list(importance.items())[:15])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create feature importance visualization\\n\",\n",
    "    \"    fig = px.bar(\\n\",\n",
    "    \"        x=list(top_features.values()),\\n\",\n",
    "    \"        y=list(top_features.keys()),\\n\",\n",
    "    \"        orientation='h',\\n\",\n",
    "    \"        title='🎯 Top 15 Most Important Features',\\n\",\n",
    "    \"        labels={'x': 'Importance Score', 'y': 'Features'},\\n\",\n",
    "    \"        color=list(top_features.values()),\\n\",\n",
    "    \"        color_continuous_scale='viridis'\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig.update_layout(\\n\",\n",
    "    \"        height=600,\\n\",\n",
    "    \"        yaxis={'categoryorder': 'total ascending'},\\n\",\n",
    "    \"        showlegend=False\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Display top features\\n\",\n",
    "    \"    print(\\\"💎 Top 5 Most Important Features:\\\")\\n\",\n",
    "    \"    for i, (feature, importance_val) in enumerate(list(importance.items())[:5], 1):\\n\",\n",
    "    \"        print(f\\\"{i}. {feature}: {importance_val:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"⚠️ Feature importance not available for this model type\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"model-explainability\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 🧠 Model Explainability with SHAP\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"shap-explainability\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate comprehensive model explanations\\n\",\n",
    "    \"print(\\\"🔍 Generating Model Explanations...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_model_name = ml_results['best_model']\\n\",\n",
    "    \"best_model = ml_results['all_models'][best_model_name]['model']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use a sample of data for faster computation\\n\",\n",
    "    \"X_sample = X.sample(n=min(500, len(X)), random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    import shap\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create SHAP explainer\\n\",\n",
    "    \"    explainer = shap.TreeExplainer(best_model)\\n\",\n",
    "    \"    shap_values = explainer.shap_values(X_sample)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"✅ SHAP analysis completed successfully!\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # SHAP summary plot\\n\",\n",
    "    \"    plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"    if isinstance(shap_values, list):\\n\",\n",
    "    \"        # For multi-class\\n\",\n",
    "    \"        shap.summary_plot(shap_values[1], X_sample, feature_names=feature_cols, show=False)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # For binary classification\\n\",\n",
    "    \"        shap.summary_plot(shap_values, X_sample, feature_names=feature_cols, show=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.title('🔍 SHAP Feature Importance Summary')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate business insights\\n\",\n",
    "    \"    print(\\\"\\\\n💡 AI-Generated Business Insights:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get top features from SHAP\\n\",\n",
    "    \"    if isinstance(shap_values, list):\\n\",\n",
    "    \"        shap_importance = np.abs(shap_values[1]).mean(0)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        shap_importance = np.abs(shap_values).mean(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    top_feature_indices = np.argsort(shap_importance)[-3:][::-1]\\n\",\n",
    "    \"    top_features = [feature_cols[i] for i in top_feature_indices]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights = [\\n\",\n",
    "    \"        f\\\"The model's predictions are primarily driven by: {', '.join(top_features)}\\\",\\n\",\n",
    "    \"        f\\\"Focus on data quality and monitoring for '{top_features[0]}' as it has the strongest influence\\\",\\n\",\n",
    "    \"        \\\"Consider feature engineering to create interactions between top features\\\",\\n\",\n",
    "    \"        \\\"Monitor these key features for concept drift in production\\\",\\n\",\n",
    "    \"        \\\"Use these insights to inform business strategy and customer interventions\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, insight in enumerate(insights, 1):\\n\",\n",
    "    \"        print(f\\\"{i}. {insight}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"⚠️ Explainability analysis failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"This is normal for some model types or in notebook environment\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"advanced-analysis\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 📈 Advanced Analysis & Model Diagnostics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"model-diagnostics\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Model diagnostics and advanced analysis\\n\",\n",
    "    \"print(\\\"📈 MODEL DIAGNOSTICS AND ADVANCED ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion Matrix for best model\\n\",\n",
    "    \"best_model_name = ml_results['best_model']\\n\",\n",
    "    \"best_model = ml_results['all_models'][best_model_name]['model']\\n\",\n",
    "    \"X_test = ml_results['predictions']['test_actual'].index\\n\",\n",
    "    \"y_test = ml_results['predictions']['test_actual']\\n\",\n",
    "    \"y_pred = ml_results['predictions']['test_predictions']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion Matrix\\n\",\n",
    "    \"cm = confusion_matrix(y_test, y_pred)\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"            xticklabels=['Not Churn', 'Churn'], \\n\",\n",
    "    \"            yticklabels=['Not Churn', 'Churn'])\\n\",\n",
    "    \"plt.title(f'📊 Confusion Matrix - {best_model_name}')\\n\",\n",
    "    \"plt.ylabel('Actual')\\n\",\n",
    "    \"plt.xlabel('Predicted')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification Report\\n\",\n",
    "    \"print(\\\"\\\\n📋 Classification Report:\\\")\\n\",\n",
    "    \"print(classification_report(y_test, y_pred, target_names=['Not Churn', 'Churn']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ROC Curve\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    y_proba = best_model.predict_proba(X.loc[X_test])[:, 1]\\n\",\n",
    "    \"    fpr, tpr, _ = roc_curve(y_test, y_proba)\\n\",\n",
    "    \"    auc_score = roc_auc_score(y_test, y_proba)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\\n\",\n",
    "    \"    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\\n\",\n",
    "    \"    plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"    plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"    plt.title('📈 ROC Curve')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"🎯 AUC Score: {auc_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save-results\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 💾 Save Results & Export Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save-model-results\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save model results and artifacts\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create results directory\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"os.makedirs('model_results', exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save the best model\\n\",\n",
    "    \"best_model_name = ml_results['best_model']\\n\",\n",
    "    \"best_model_obj = ml_results['all_models'][best_model_name]['model']\\n\",\n",
    "    \"\\n\",\n",
    "    \"joblib.dump(best_model_obj, f'model_results/best_model_{best_model_name}.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save results summary\\n\",\n",
    "    \"results_summary = {\\n\",\n",
    "    \"    'timestamp': datetime.now().isoformat(),\\n\",\n",
    "    \"    'best_model': best_model_name,\\n\",\n",
    "    \"    'best_score': ml_results['test_metrics'],\\n\",\n",
    "    \"    'training_time_seconds': training_time,\\n\",\n",
    "    \"    'dataset_info': ml_results['data_info'],\\n\",\n",
    "    \"    'all_models_performance': {\\n\",\n",
    "    \"        name: {\\n\",\n",
    "    \"            'score': results['primary_score'],\\n\",\n",
    "    \"            'training_time': results['training_time'],\\n\",\n",
    "    \"            'framework': ml_module._get_model_framework(name)\\n\",\n",
    "    \"        } for name, results in ml_results['all_models'].items()\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('model_results/training_summary.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(results_summary, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"💾 Results saved successfully!\\\")\\n\",\n",
    "    \"print(f\\\"   - Best model: model_results/best_model_{best_model_name}.pkl\\\")\\n\",\n",
    "    \"print(f\\\"   - Summary: model_results/training_summary.json\\\")\\n\",\n",
    "    \"print(f\\\"   - Training time: {training_time:.2f} seconds\\\")\\n\",\n",
    "    \"print(f\\\"   - Models trained: {len(ml_results['all_models'])}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"key-insights\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 🎯 Key Insights & Recommendations\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"generate-key-insights\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate key insights\\n\",\n",
    "    \"print(\\\"🎯 KEY INSIGHTS & RECOMMENDATIONS\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Model performance insights\\n\",\n",
    "    \"best_score = float(ml_results['all_models'][best_model_name]['primary_score'])\\n\",\n",
    "    \"performance_tier = \\\"Excellent\\\" if best_score > 0.9 else \\\"Good\\\" if best_score > 0.8 else \\\"Moderate\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n📊 Performance Summary:\\\")\\n\",\n",
    "    \"print(f\\\"   • Best Model: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"   • Performance: {best_score:.4f} ({performance_tier})\\\")\\n\",\n",
    "    \"print(f\\\"   • Training Efficiency: {training_time:.2f} seconds\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Framework insights\\n\",\n",
    "    \"frameworks = [ml_module._get_model_framework(m) for m in ml_results['all_models'].keys()]\\n\",\n",
    "    \"framework_counts = pd.Series(frameworks).value_counts()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n🤖 Framework Usage:\\\")\\n\",\n",
    "    \"for framework, count in framework_counts.items():\\n\",\n",
    "    \"    print(f\\\"   • {framework}: {count} models\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature insights\\n\",\n",
    "    \"if 'feature_importance' in ml_results and ml_results['feature_importance']:\\n\",\n",
    "    \"    top_feature = list(ml_results['feature_importance'].keys())[0]\\n\",\n",
    "    \"    print(f\\\"\\\\n🔍 Key Driver: '{top_feature}' is the most important feature\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Business recommendations\\n\",\n",
    "    \"print(f\\\"\\\\n💡 Business Recommendations:\\\")\\n\",\n",
    "    \"print(f\\\"   1. Use {best_model_name} for production deployment\\\")\\n\",\n",
    "    \"print(f\\\"   2. Monitor key features for data quality and concept drift\\\")\\n\",\n",
    "    \"print(f\\\"   3. Implement automated retraining pipeline\\\")\\n\",\n",
    "    \"print(f\\\"   4. Set up model performance monitoring dashboard\\\")\\n\",\n",
    "    \"print(f\\\"   5. Use SHAP explanations for business stakeholder communication\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Technical recommendations\\n\",\n",
    "    \"print(f\\\"\\\\n🔧 Technical Recommendations:\\\")\\n\",\n",
    "    \"print(f\\\"   1. Consider hyperparameter tuning for further improvement\\\")\\n\",\n",
    "    \"print(f\\\"   2. Explore ensemble methods for better performance\\\")\\n\",\n",
    "    \"print(f\\\"   3. Implement feature store for consistent feature engineering\\\")\\n\",\n",
    "    \"print(f\\\"   4. Set up ML pipeline with version control\\\")\\n\",\n",
    "    \"print(f\\\"   5. Monitor model fairness and bias\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\n\",\n",
    "    \"print(\\\"✅ Model Training Notebook Completed Successfully!\\\")\\n\",\n",
    "    \"print(\\\"🚀 Ready for production deployment and further analysis\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"next-steps\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## 📋 Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Immediate Actions:\\n\",\n",
    "    \"1. **Deploy Best Model**: Use the saved model for predictions\\n\",\n",
    "    \"2. **Monitor Performance**: Set up tracking for model drift\\n\",\n",
    "    \"3. **Feature Monitoring**: Track important feature distributions\\n\",\n",
    "    \"4. **A/B Testing**: Test model performance in production\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Advanced Analysis:\\n\",\n",
    "    \"1. **Hyperparameter Tuning**: Further optimize model parameters\\n\",\n",
    "    \"2. **Ensemble Methods**: Combine multiple models for better performance\\n\",\n",
    "    \"3. **Advanced Feature Engineering**: Create more sophisticated features\\n\",\n",
    "    \"4. **Time Series Analysis**: If applicable, analyze temporal patterns\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Production Readiness:\\n\",\n",
    "    \"1. **API Development**: Create prediction endpoints\\n\",\n",
    "    \"2. **Monitoring Dashboard**: Track model performance\\n\",\n",
    "    \"3. **Automated Retraining**: Set up pipeline for model updates\\n\",\n",
    "    \"4. **Documentation**: Create model cards and documentation\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"provenance\": [],\n",
    "   \"gpuType\": \"T4\"\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  },\n",
    "  \"accelerator\": \"GPU\"\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
