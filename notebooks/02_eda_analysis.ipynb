{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"eda-analysis\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# üîç Exploratory Data Analysis - Deep Dive\\n\",\n",
    "    \"\\n\",\n",
    "    \"Comprehensive EDA analysis using the GenAI Autonomous Data Agent framework.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"install-dependencies\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages\\n\",\n",
    "    \"!pip install -q pandas numpy scikit-learn plotly matplotlib seaborn\\n\",\n",
    "    \"!pip install -q sweetviz pandas-profiling\\n\",\n",
    "    \"!pip install -q autoviz\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ All EDA packages installed successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"import-libraries\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Enhanced EDA Module\\n\",\n",
    "    \"class EDAModule:\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        self.analysis_results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def generate_comprehensive_eda(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Generate comprehensive EDA analysis\\\"\\\"\\\"\\n\",\n",
    "    \"        results = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Basic data overview\\n\",\n",
    "    \"        results['data_overview'] = self._get_data_overview(df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Statistical summary\\n\",\n",
    "    \"        results['statistical_summary'] = self._get_statistical_summary(df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Correlation analysis\\n\",\n",
    "    \"        results['correlation_analysis'] = self._get_correlation_analysis(df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Missing values analysis\\n\",\n",
    "    \"        results['missing_analysis'] = self._get_missing_analysis(df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # AI-generated insights\\n\",\n",
    "    \"        results['insights'] = self._generate_ai_insights(df, results)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_data_overview(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Get basic data overview\\\"\\\"\\\"\\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'basic_info': {\\n\",\n",
    "    \"                'total_rows': len(df),\\n\",\n",
    "    \"                'total_columns': len(df.columns),\\n\",\n",
    "    \"                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\\n\",\n",
    "    \"                'data_types': dict(df.dtypes.value_counts())\\n\",\n",
    "    \"            },\\n\",\n",
    "    \"            'column_info': {\\n\",\n",
    "    \"                'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),\\n\",\n",
    "    \"                'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),\\n\",\n",
    "    \"                'datetime_columns': df.select_dtypes(include=['datetime']).columns.tolist()\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_statistical_summary(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Get statistical summary\\\"\\\"\\\"\\n\",\n",
    "    \"        return df.describe(include='all').round(4)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_correlation_analysis(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Perform correlation analysis\\\"\\\"\\\"\\n\",\n",
    "    \"        numeric_df = df.select_dtypes(include=[np.number])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(numeric_df.columns) < 2:\\n\",\n",
    "    \"            return {'error': 'Not enough numeric columns for correlation analysis'}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        corr_matrix = numeric_df.corr()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Find high correlations\\n\",\n",
    "    \"        high_correlations = []\\n\",\n",
    "    \"        for i in range(len(corr_matrix.columns)):\\n\",\n",
    "    \"            for j in range(i+1, len(corr_matrix.columns)):\\n\",\n",
    "    \"                corr_val = corr_matrix.iloc[i, j]\\n\",\n",
    "    \"                if abs(corr_val) > 0.7:\\n\",\n",
    "    \"                    high_correlations.append({\\n\",\n",
    "    \"                        'variable1': corr_matrix.columns[i],\\n\",\n",
    "    \"                        'variable2': corr_matrix.columns[j],\\n\",\n",
    "    \"                        'correlation': corr_val\\n\",\n",
    "    \"                    })\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'correlation_matrix': corr_matrix,\\n\",\n",
    "    \"            'high_correlations': high_correlations,\\n\",\n",
    "    \"            'strongest_correlation': max([abs(corr_matrix.iloc[i, j]) \\n\",\n",
    "    \"                                        for i in range(len(corr_matrix.columns)) \\n\",\n",
    "    \"                                        for j in range(i+1, len(corr_matrix.columns))], \\n\",\n",
    "    \"                                        default=0)\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_missing_analysis(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Analyze missing values\\\"\\\"\\\"\\n\",\n",
    "    \"        missing_count = df.isnull().sum()\\n\",\n",
    "    \"        missing_percentage = (missing_count / len(df)) * 100\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'missing_count': missing_count[missing_count > 0].to_dict(),\\n\",\n",
    "    \"            'missing_percentage': missing_percentage[missing_percentage > 0].to_dict(),\\n\",\n",
    "    \"            'total_missing': missing_count.sum(),\\n\",\n",
    "    \"            'missing_percentage_total': (missing_count.sum() / (len(df) * len(df.columns))) * 100\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _generate_ai_insights(self, df, results):\\n\",\n",
    "    \"        \\\"\\\"\\\"Generate AI-powered insights\\\"\\\"\\\"\\n\",\n",
    "    \"        insights = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Data quality insights\\n\",\n",
    "    \"        missing_info = results['missing_analysis']\\n\",\n",
    "    \"        if missing_info['total_missing'] > 0:\\n\",\n",
    "    \"            insights.append(f\\\"Dataset contains {missing_info['total_missing']} missing values ({missing_info['missing_percentage_total']:.1f}%). Consider imputation strategies.\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            insights.append(\\\"Excellent data quality - no missing values detected.\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Correlation insights\\n\",\n",
    "    \"        corr_info = results['correlation_analysis']\\n\",\n",
    "    \"        if 'high_correlations' in corr_info and corr_info['high_correlations']:\\n\",\n",
    "    \"            insights.append(f\\\"Found {len(corr_info['high_correlations'])} highly correlated feature pairs. Consider feature selection to reduce multicollinearity.\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Data size insights\\n\",\n",
    "    \"        overview = results['data_overview']\\n\",\n",
    "    \"        insights.append(f\\\"Dataset has {overview['basic_info']['total_rows']:,} rows and {overview['basic_info']['total_columns']} features - suitable for machine learning.\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Feature type insights\\n\",\n",
    "    \"        col_info = overview['column_info']\\n\",\n",
    "    \"        insights.append(f\\\"Numeric features: {len(col_info['numeric_columns'])}, Categorical features: {len(col_info['categorical_columns'])}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Statistical insights\\n\",\n",
    "    \"        stats = results['statistical_summary']\\n\",\n",
    "    \"        if 'customer_age' in df.columns:\\n\",\n",
    "    \"            age_mean = df['customer_age'].mean()\\n\",\n",
    "    \"            insights.append(f\\\"Average customer age: {age_mean:.1f} years\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if 'annual_income' in df.columns:\\n\",\n",
    "    \"            income_std = df['annual_income'].std()\\n\",\n",
    "    \"            insights.append(f\\\"Income variability: ${income_std:,.0f} standard deviation\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return insights\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def create_correlation_heatmap(self, df):\\n\",\n",
    "    \"        \\\"\\\"\\\"Create correlation heatmap\\\"\\\"\\\"\\n\",\n",
    "    \"        numeric_df = df.select_dtypes(include=[np.number])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(numeric_df.columns) < 2:\\n\",\n",
    "    \"            fig = go.Figure()\\n\",\n",
    "    \"            fig.add_annotation(text=\\\"Not enough numeric columns for correlation matrix\\\")\\n\",\n",
    "    \"            return fig\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        corr_matrix = numeric_df.corr()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig = px.imshow(corr_matrix,\\n\",\n",
    "    \"                       title=\\\"üìä Feature Correlation Matrix\\\",\\n\",\n",
    "    \"                       color_continuous_scale='RdBu_r',\\n\",\n",
    "    \"                       aspect=\\\"auto\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=600)\\n\",\n",
    "    \"        return fig\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def create_distribution_plot(self, df, column):\\n\",\n",
    "    \"        \\\"\\\"\\\"Create distribution plot for a column\\\"\\\"\\\"\\n\",\n",
    "    \"        if column not in df.columns:\\n\",\n",
    "    \"            fig = go.Figure()\\n\",\n",
    "    \"            fig.add_annotation(text=f\\\"Column '{column}' not found\\\")\\n\",\n",
    "    \"            return fig\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if df[column].dtype in ['object', 'category']:\\n\",\n",
    "    \"            # Categorical data\\n\",\n",
    "    \"            value_counts = df[column].value_counts().head(10)\\n\",\n",
    "    \"            fig = px.bar(x=value_counts.index, y=value_counts.values,\\n\",\n",
    "    \"                        title=f\\\"üìà Distribution of {column}\\\",\\n\",\n",
    "    \"                        labels={'x': column, 'y': 'Count'})\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Numerical data\\n\",\n",
    "    \"            fig = px.histogram(df, x=column, \\n\",\n",
    "    \"                             title=f\\\"üìä Distribution of {column}\\\",\\n\",\n",
    "    \"                             marginal=\\\"box\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=400)\\n\",\n",
    "    \"        return fig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize EDA module\\n\",\n",
    "    \"eda = EDAModule()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîç EDA Analysis notebook initialized!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"load-sample-dataset\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìä Load Sample Dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"generate-sample-data\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate comprehensive e-commerce dataset\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"n_samples = 5000\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = {\\n\",\n",
    "    \"    'customer_id': range(1, n_samples + 1),\\n\",\n",
    "    \"    'age': np.random.normal(35, 10, n_samples).astype(int),\\n\",\n",
    "    \"    'income': np.random.lognormal(10.5, 0.8, n_samples),\\n\",\n",
    "    \"    'credit_score': np.random.normal(650, 100, n_samples).astype(int),\\n\",\n",
    "    \"    'months_active': np.random.exponential(24, n_samples).astype(int),\\n\",\n",
    "    \"    'total_purchases': np.random.poisson(15, n_samples),\\n\",\n",
    "    \"    'avg_order_value': np.random.gamma(2, 50, n_samples),\\n\",\n",
    "    \"    'days_since_last_purchase': np.random.exponential(30, n_samples).astype(int),\\n\",\n",
    "    \"    'website_visits_month': np.random.poisson(8, n_samples),\\n\",\n",
    "    \"    'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'], n_samples, p=[0.2, 0.5, 0.3]),\\n\",\n",
    "    \"    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\\n\",\n",
    "    \"    'preferred_category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Beauty'], n_samples),\\n\",\n",
    "    \"    'satisfaction_score': np.random.randint(1, 6, n_samples),\\n\",\n",
    "    \"    'churn_risk': np.random.beta(2, 5, n_samples)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create target variable\\n\",\n",
    "    \"data['high_value'] = (\\n\",\n",
    "    \"    (data['income'] > 80000) & \\n\",\n",
    "    \"    (data['avg_order_value'] > 100) & \\n\",\n",
    "    \"    (data['total_purchases'] > 10)\\n\",\n",
    "    \").astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create DataFrame\\n\",\n",
    "    \"df = pd.DataFrame(data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Ensure realistic ranges\\n\",\n",
    "    \"df['age'] = np.clip(df['age'], 18, 80)\\n\",\n",
    "    \"df['income'] = np.clip(df['income'], 20000, 200000)\\n\",\n",
    "    \"df['credit_score'] = np.clip(df['credit_score'], 300, 850)\\n\",\n",
    "    \"df['churn_risk'] = np.clip(df['churn_risk'], 0, 1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Dataset created: {df.shape[0]} rows, {df.shape[1]} columns\\\")\\n\",\n",
    "    \"print(f\\\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\\")\\n\",\n",
    "    \"print(f\\\"High-value customers: {df['high_value'].sum()} ({df['high_value'].mean()*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"comprehensive-eda\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üéØ Comprehensive EDA Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run-comprehensive-eda\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run comprehensive EDA\\n\",\n",
    "    \"eda_results = eda.generate_comprehensive_eda(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Comprehensive EDA completed!\\\")\\n\",\n",
    "    \"print(f\\\"Analysis sections: {list(eda_results.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"display-statistical-summary\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display statistical summary\\n\",\n",
    "    \"statistical_summary = eda_results['statistical_summary']\\n\",\n",
    "    \"print(\\\"üìä Statistical Summary:\\\")\\n\",\n",
    "    \"statistical_summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"visualize-correlations\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize correlations\\n\",\n",
    "    \"correlation_fig = eda.create_correlation_heatmap(df)\\n\",\n",
    "    \"correlation_fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"distribution-analysis\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Distribution analysis for key numeric variables\\n\",\n",
    "    \"numeric_cols = df.select_dtypes(include=[np.number]).columns[:6]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìà Distribution Analysis for Key Numeric Variables:\\\")\\n\",\n",
    "    \"for col in numeric_cols:\\n\",\n",
    "    \"    if col != 'customer_id':  # Skip ID column\\n\",\n",
    "    \"        fig = eda.create_distribution_plot(df, col)\\n\",\n",
    "    \"        fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"categorical-analysis\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Categorical variable analysis\\n\",\n",
    "    \"categorical_cols = df.select_dtypes(include=['object']).columns\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Categorical Variable Analysis:\\\")\\n\",\n",
    "    \"for col in categorical_cols:\\n\",\n",
    "    \"    fig = eda.create_distribution_plot(df, col)\\n\",\n",
    "    \"    fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ai-generated-insights\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üí° AI-Generated Insights\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"display-ai-insights\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display AI insights\\n\",\n",
    "    \"insights = eda_results['insights']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ü§ñ AI-Generated Insights:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"for i, insight in enumerate(insights, 1):\\n\",\n",
    "    \"    print(f\\\"{i}. {insight}\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"summary-report\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìã Summary Report\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"create-summary-report\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create summary report\\n\",\n",
    "    \"data_overview = eda_results['data_overview']\\n\",\n",
    "    \"correlation_analysis = eda_results['correlation_analysis']\\n\",\n",
    "    \"missing_analysis = eda_results['missing_analysis']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìã EDA SUMMARY REPORT\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"print(f\\\"üìä Dataset Size: {data_overview['basic_info']['total_rows']:,} rows\\\")\\n\",\n",
    "    \"print(f\\\"üî¢ Features: {data_overview['basic_info']['total_columns']}\\\")\\n\",\n",
    "    \"print(f\\\"üíæ Memory Usage: {data_overview['basic_info']['memory_usage_mb']:.1f} MB\\\")\\n\",\n",
    "    \"print(f\\\"‚ùå Missing Values: {missing_analysis['total_missing']} ({missing_analysis['missing_percentage_total']:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìà Feature Types:\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Numeric: {len(data_overview['column_info']['numeric_columns'])}\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Categorical: {len(data_overview['column_info']['categorical_columns'])}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'high_correlations' in correlation_analysis:\\n\",\n",
    "    \"    print(f\\\"\\\\nüîó High Correlations Found: {len(correlation_analysis['high_correlations'])}\\\")\\n\",\n",
    "    \"    for corr in correlation_analysis['high_correlations'][:3]:\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {corr['variable1']} ‚Üî {corr['variable2']}: {corr['correlation']:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüéØ Target Variable (high_value):\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Positive class: {df['high_value'].sum()} ({df['high_value'].mean()*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Class balance: {'Balanced' if 0.4 < df['high_value'].mean() < 0.6 else 'Imbalanced'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n‚úÖ EDA Complete - Ready for Feature Engineering and ML!\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"advanced-visualizations\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Advanced visualizations\\n\",\n",
    "    \"print(\\\"üé® Advanced Visualizations\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Income vs Age colored by high_value\\n\",\n",
    "    \"fig1 = px.scatter(df, x='age', y='income', color='high_value',\\n\",\n",
    "    \"                 title='üí∞ Income vs Age (Colored by High Value Customer)',\\n\",\n",
    "    \"                 labels={'age': 'Age', 'income': 'Annual Income'},\\n\",\n",
    "    \"                 color_continuous_scale='viridis')\\n\",\n",
    "    \"fig1.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Customer segments by high value\\n\",\n",
    "    \"segment_high_value = df.groupby('customer_segment')['high_value'].mean().reset_index()\\n\",\n",
    "    \"fig2 = px.bar(segment_high_value, x='customer_segment', y='high_value',\\n\",\n",
    "    \"             title='üë• High Value Customers by Segment',\\n\",\n",
    "    \"             labels={'customer_segment': 'Customer Segment', 'high_value': 'High Value Proportion'})\\n\",\n",
    "    \"fig2.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Regional distribution\\n\",\n",
    "    \"region_counts = df['region'].value_counts()\\n\",\n",
    "    \"fig3 = px.pie(values=region_counts.values, names=region_counts.index,\\n\",\n",
    "    \"             title='üåç Customer Distribution by Region')\\n\",\n",
    "    \"fig3.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"data-quality-assessment\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üîç Data Quality Assessment\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"data-quality-metrics\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data quality metrics\\n\",\n",
    "    \"def assess_data_quality(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Comprehensive data quality assessment\\\"\\\"\\\"\\n\",\n",
    "    \"    quality_metrics = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Completeness\\n\",\n",
    "    \"    completeness = 1 - (df.isnull().sum().sum() / (df.shape[0] * df.shape[1]))\\n\",\n",
    "    \"    quality_metrics['completeness'] = completeness\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Uniqueness\\n\",\n",
    "    \"    uniqueness = df.nunique() / len(df)\\n\",\n",
    "    \"    quality_metrics['avg_uniqueness'] = uniqueness.mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Consistency (check for outliers in numeric columns)\\n\",\n",
    "    \"    numeric_cols = df.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"    outlier_rates = {}\\n\",\n",
    "    \"    for col in numeric_cols:\\n\",\n",
    "    \"        Q1 = df[col].quantile(0.25)\\n\",\n",
    "    \"        Q3 = df[col].quantile(0.75)\\n\",\n",
    "    \"        IQR = Q3 - Q1\\n\",\n",
    "    \"        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).mean()\\n\",\n",
    "    \"        outlier_rates[col] = outliers\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    quality_metrics['avg_outlier_rate'] = np.mean(list(outlier_rates.values()))\\n\",\n",
    "    \"    quality_metrics['outlier_rates'] = outlier_rates\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return quality_metrics\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run data quality assessment\\n\",\n",
    "    \"quality_metrics = assess_data_quality(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîç DATA QUALITY ASSESSMENT\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"üìä Completeness: {quality_metrics['completeness']:.1%}\\\")\\n\",\n",
    "    \"print(f\\\"üéØ Uniqueness: {quality_metrics['avg_uniqueness']:.1%}\\\")\\n\",\n",
    "    \"print(f\\\"üìà Outlier Rate: {quality_metrics['avg_outlier_rate']:.1%}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüìã Quality Rating:\\\", end=\\\" \\\")\\n\",\n",
    "    \"if quality_metrics['completeness'] > 0.95 and quality_metrics['avg_outlier_rate'] < 0.05:\\n\",\n",
    "    \"    print(\\\"‚úÖ EXCELLENT\\\")\\n\",\n",
    "    \"elif quality_metrics['completeness'] > 0.90 and quality_metrics['avg_outlier_rate'] < 0.10:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è GOOD\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ùå NEEDS IMPROVEMENT\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüéØ Ready for Machine Learning Training!\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"provenance\": []\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
